{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load PySpark\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(master=\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-F09JRR8.attlocal.net:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=pyspark-shell>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# USing Spark UI\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark- Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accumulator',\n",
       " 'AccumulatorParam',\n",
       " 'BarrierTaskContext',\n",
       " 'BarrierTaskInfo',\n",
       " 'BasicProfiler',\n",
       " 'Broadcast',\n",
       " 'HiveContext',\n",
       " 'InheritableThread',\n",
       " 'MarshalSerializer',\n",
       " 'PickleSerializer',\n",
       " 'Profiler',\n",
       " 'RDD',\n",
       " 'RDDBarrier',\n",
       " 'Row',\n",
       " 'SQLContext',\n",
       " 'SparkConf',\n",
       " 'SparkContext',\n",
       " 'SparkFiles',\n",
       " 'SparkJobInfo',\n",
       " 'SparkStageInfo',\n",
       " 'StatusTracker',\n",
       " 'StorageLevel',\n",
       " 'TaskContext',\n",
       " '_NoValue',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_globals',\n",
       " 'accumulators',\n",
       " 'broadcast',\n",
       " 'cloudpickle',\n",
       " 'conf',\n",
       " 'context',\n",
       " 'copy_func',\n",
       " 'files',\n",
       " 'find_spark_home',\n",
       " 'inheritable_thread_target',\n",
       " 'java_gateway',\n",
       " 'join',\n",
       " 'keyword_only',\n",
       " 'ml',\n",
       " 'profiler',\n",
       " 'rdd',\n",
       " 'rddsampler',\n",
       " 'resource',\n",
       " 'resultiterable',\n",
       " 'serializers',\n",
       " 'shuffle',\n",
       " 'since',\n",
       " 'sql',\n",
       " 'statcounter',\n",
       " 'status',\n",
       " 'storagelevel',\n",
       " 'taskcontext',\n",
       " 'traceback_utils',\n",
       " 'types',\n",
       " 'util',\n",
       " 'version',\n",
       " 'wraps']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to Review Methods / Attrib in pyspark\n",
    "dir(pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession #import the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session for DF\n",
    "spark = SparkSession.builder.appName(\"MLwithSpark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyspark\n",
    "#from pyspark.sql import SparkSession\n",
    "#spark = SparkSession.builder.master(\"local\").appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 (8 points): The goal of this step is to read the data from the local folder. You should infer the schema (e.g., columns) from the csv file. Tip: explore the spark_csv package from databricks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I) Read the data from the local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file without header/Schema\n",
    "df = spark.read.csv(\"C:\\\\Users\\\\Zed Kamurase\\\\Desktop\\\\MSBA22\\\\SPRING JAN-MAY 2022\\\\1. ISOM-676-4101 Machine Learning II\\\\Assignments\\\\2\\\\titanic\\\\train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|        _c0|     _c1|   _c2|                 _c3|   _c4|_c5|  _c6|  _c7|             _c8|    _c9| _c10|    _c11|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male| 22|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female| 38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female| 26|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female| 35|    1|    0|          113803|   53.1| C123|       S|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview Data\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer with schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read CSV file with header/Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\n",
    "    path=\"C:\\\\Users\\\\Zed Kamurase\\\\Desktop\\\\MSBA22\\\\SPRING JAN-MAY 2022\\\\1. ISOM-676-4101 Machine Learning II\\\\Assignments\\\\2\\\\titanic\\\\train.csv\",\n",
    "    header=True,\n",
    "    quote='\"',\n",
    "    inferSchema=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : The goal of this step is to familiarize yourself with the dataset. This step is useful in detecting data problems, informing the data engineering steps, and informing the feature selection processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dataset and verify that the schema contains all the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I)Print the first 10 rows from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II)Obtain summary statistics for all variables in the dataframe. Pay attention to whether there are\n",
    "missing data as well as whether the field appears to be continuous or discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii)\tPrint the first 10 rows from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|summary|      PassengerId|           Survived|            Pclass|                Name|   Sex|               Age|             SibSp|              Parch|            Ticket|             Fare|Cabin|Embarked|\n",
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|  count|              891|                891|               891|                 891|   891|               714|               891|                891|               891|              891|  204|     889|\n",
      "|   mean|            446.0| 0.3838383838383838| 2.308641975308642|                null|  null| 29.69911764705882|0.5230078563411896|0.38159371492704824|260318.54916792738| 32.2042079685746| null|    null|\n",
      "| stddev|257.3538420152301|0.48659245426485753|0.8360712409770491|                null|  null|14.526497332334035|1.1027434322934315| 0.8060572211299488|471609.26868834975|49.69342859718089| null|    null|\n",
      "|    min|                1|                  0|                 1|\"Andersson, Mr. A...|female|              0.42|                 0|                  0|            110152|              0.0|  A10|       C|\n",
      "|    max|              891|                  1|                 3|van Melkebeke, Mr...|  male|              80.0|                 8|                  6|         WE/P 5735|         512.3292|    T|       S|\n",
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iv)\tFor each of the string columns (except name and ticket), print the count of the 10 most frequent values ordered by descending order of frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "===========================================\n",
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)\n",
    "## Get Datatype of each columns\n",
    "print('===========================================')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'Survived',\n",
       " 'Pclass',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Ticket',\n",
       " 'Fare',\n",
       " 'Cabin',\n",
       " 'Embarked']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iv)\tFor each of the string columns (except name and ticket), print the count of the 10 most frequent values ordered by descending order of frequency.\n",
    "* Drop Columns (Name and ticket) and name it as new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.drop('Name','Ticket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+------+----+-----+-----+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|   Sex| Age|SibSp|Parch|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+------+----+-----+-----+-------+-----+--------+\n",
      "|        891|       0|     3|  male|32.0|    0|    0|   7.75| null|       Q|\n",
      "|        890|       1|     1|  male|26.0|    0|    0|   30.0| C148|       C|\n",
      "|        889|       0|     3|female|null|    1|    2|  23.45| null|       S|\n",
      "|        888|       1|     1|female|19.0|    0|    0|   30.0|  B42|       S|\n",
      "|        887|       0|     2|  male|27.0|    0|    0|   13.0| null|       S|\n",
      "|        886|       0|     3|female|39.0|    0|    5| 29.125| null|       Q|\n",
      "|        885|       0|     3|  male|25.0|    0|    0|   7.05| null|       S|\n",
      "|        884|       0|     2|  male|28.0|    0|    0|   10.5| null|       S|\n",
      "|        883|       0|     3|female|22.0|    0|    0|10.5167| null|       S|\n",
      "|        882|       0|     3|  male|33.0|    0|    0| 7.8958| null|       S|\n",
      "+-----------+--------+------+------+----+-----+-----+-------+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.sort('PassengerId','Survived','Pclass','Sex','Age','SibSp','Parch','Fare','Cabin','Embarked', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V)I have kept the data type that is not string, columns with string data type are not insightful in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3 The goal of this step is to engineer the necessary features for the machine learning model.\n",
    "Logistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i)\tSelect all feature columns you plan to use in addition to the target variable (i.e., ‘Survived’) and covert all numerical columns into double data type. Tip: you can use the. cast() from pyspark.sql.functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+------+----+-----+-----+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|   Sex| Age|SibSp|Parch|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+------+----+-----+-----+-------+-----+--------+\n",
      "|          1|       0|     3|  male|22.0|    1|    0|   7.25| null|       S|\n",
      "|          2|       1|     1|female|38.0|    1|    0|71.2833|  C85|       C|\n",
      "|          3|       1|     3|female|26.0|    0|    0|  7.925| null|       S|\n",
      "|          4|       1|     1|female|35.0|    1|    0|   53.1| C123|       S|\n",
      "|          5|       0|     3|  male|35.0|    0|    0|   8.05| null|       S|\n",
      "+-----------+--------+------+------+----+-----+-----+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "         *  ========I dropped Cabin and Embarked columns Bz they have string data types====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+----+-----+-----+-------+--------+\n",
      "|PassengerId|Pclass|   Sex| Age|SibSp|Parch|   Fare|Survived|\n",
      "+-----------+------+------+----+-----+-----+-------+--------+\n",
      "|          1|     3|  male|22.0|    1|    0|   7.25|       0|\n",
      "|          2|     1|female|38.0|    1|    0|71.2833|       1|\n",
      "|          3|     3|female|26.0|    0|    0|  7.925|       1|\n",
      "|          4|     1|female|35.0|    1|    0|   53.1|       1|\n",
      "|          5|     3|  male|35.0|    0|    0|   8.05|       0|\n",
      "|          6|     3|  male|null|    0|    0| 8.4583|       0|\n",
      "|          7|     1|  male|54.0|    0|    0|51.8625|       0|\n",
      "|          8|     3|  male| 2.0|    3|    1| 21.075|       0|\n",
      "|          9|     3|female|27.0|    0|    2|11.1333|       1|\n",
      "|         10|     2|female|14.0|    1|    0|30.0708|       1|\n",
      "|         11|     3|female| 4.0|    1|    1|   16.7|       1|\n",
      "|         12|     1|female|58.0|    0|    0|  26.55|       1|\n",
      "|         13|     3|  male|20.0|    0|    0|   8.05|       0|\n",
      "|         14|     3|  male|39.0|    1|    5| 31.275|       0|\n",
      "|         15|     3|female|14.0|    0|    0| 7.8542|       0|\n",
      "|         16|     2|female|55.0|    0|    0|   16.0|       1|\n",
      "|         17|     3|  male| 2.0|    4|    1| 29.125|       0|\n",
      "|         18|     2|  male|null|    0|    0|   13.0|       1|\n",
      "|         19|     3|female|31.0|    1|    0|   18.0|       0|\n",
      "|         20|     3|female|null|    0|    0|  7.225|       1|\n",
      "+-----------+------+------+----+-----+-----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = new_df.select('PassengerId','Pclass','Sex','Age','SibSp','Parch','Fare', 'Survived')\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii)\tReplace the missing values in the Age column with the mean value. Also create a new variable (e.g., ‘AgeNA’) indicating whether the value of age was missing or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ML pkgs\n",
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+----+-----+-----+-------+--------+\n",
      "|PassengerId|Pclass|   Sex| Age|SibSp|Parch|   Fare|Survived|\n",
      "+-----------+------+------+----+-----+-----+-------+--------+\n",
      "|          1|     3|  male|22.0|    1|    0|   7.25|       0|\n",
      "|          2|     1|female|38.0|    1|    0|71.2833|       1|\n",
      "|          3|     3|female|26.0|    0|    0|  7.925|       1|\n",
      "|          4|     1|female|35.0|    1|    0|   53.1|       1|\n",
      "|          5|     3|  male|35.0|    0|    0|   8.05|       0|\n",
      "|          6|     3|  male|null|    0|    0| 8.4583|       0|\n",
      "|          7|     1|  male|54.0|    0|    0|51.8625|       0|\n",
      "|          8|     3|  male| 2.0|    3|    1| 21.075|       0|\n",
      "|          9|     3|female|27.0|    0|    2|11.1333|       1|\n",
      "|         10|     2|female|14.0|    1|    0|30.0708|       1|\n",
      "+-----------+------+------+----+-----+-----+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling the missing to the new column AgeNA if values available or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "imptr = Imputer(inputCol=('Age'), outputCol=('Age')).setStrategy('mean')\n",
    "filled_df = imptr.fit(dataset).transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+-----------------+-----+-----+-------+--------+\n",
      "|PassengerId|Pclass|   Sex|              Age|SibSp|Parch|   Fare|Survived|\n",
      "+-----------+------+------+-----------------+-----+-----+-------+--------+\n",
      "|          1|     3|  male|             22.0|    1|    0|   7.25|       0|\n",
      "|          2|     1|female|             38.0|    1|    0|71.2833|       1|\n",
      "|          3|     3|female|             26.0|    0|    0|  7.925|       1|\n",
      "|          4|     1|female|             35.0|    1|    0|   53.1|       1|\n",
      "|          5|     3|  male|             35.0|    0|    0|   8.05|       0|\n",
      "|          6|     3|  male|29.69911764705882|    0|    0| 8.4583|       0|\n",
      "|          7|     1|  male|             54.0|    0|    0|51.8625|       0|\n",
      "|          8|     3|  male|              2.0|    3|    1| 21.075|       0|\n",
      "|          9|     3|female|             27.0|    0|    2|11.1333|       1|\n",
      "|         10|     2|female|             14.0|    1|    0|30.0708|       1|\n",
      "+-----------+------+------+-----------------+-----+-----+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filled_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Adding new variables AgeNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+----+-----+-----+-------+--------+-----+\n",
      "|PassengerId|Pclass|   Sex| Age|SibSp|Parch|   Fare|Survived|AgeNA|\n",
      "+-----------+------+------+----+-----+-----+-------+--------+-----+\n",
      "|          1|     3|  male|22.0|    1|    0|   7.25|       0| 22.0|\n",
      "|          2|     1|female|38.0|    1|    0|71.2833|       1| 38.0|\n",
      "|          3|     3|female|26.0|    0|    0|  7.925|       1| 26.0|\n",
      "|          4|     1|female|35.0|    1|    0|   53.1|       1| 35.0|\n",
      "|          5|     3|  male|35.0|    0|    0|   8.05|       0| 35.0|\n",
      "|          6|     3|  male|null|    0|    0| 8.4583|       0| null|\n",
      "|          7|     1|  male|54.0|    0|    0|51.8625|       0| 54.0|\n",
      "|          8|     3|  male| 2.0|    3|    1| 21.075|       0|  2.0|\n",
      "|          9|     3|female|27.0|    0|    2|11.1333|       1| 27.0|\n",
      "|         10|     2|female|14.0|    1|    0|30.0708|       1| 14.0|\n",
      "+-----------+------+------+----+-----+-----+-------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = dataset.withColumn(\"AgeNA\", dataset['Age'])\n",
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+----+-----+-----+-------+--------+-----+\n",
      "|PassengerId|Pclass|   Sex| Age|SibSp|Parch|   Fare|Survived|AgeNA|\n",
      "+-----------+------+------+----+-----+-----+-------+--------+-----+\n",
      "|          1|     3|  male|22.0|    1|    0|   7.25|       0| 22.0|\n",
      "|          2|     1|female|38.0|    1|    0|71.2833|       1| 38.0|\n",
      "|          3|     3|female|26.0|    0|    0|  7.925|       1| 26.0|\n",
      "|          4|     1|female|35.0|    1|    0|   53.1|       1| 35.0|\n",
      "|          5|     3|  male|35.0|    0|    0|   8.05|       0| 35.0|\n",
      "|          6|     3|  male|null|    0|    0| 8.4583|       0| null|\n",
      "|          7|     1|  male|54.0|    0|    0|51.8625|       0| 54.0|\n",
      "|          8|     3|  male| 2.0|    3|    1| 21.075|       0|  2.0|\n",
      "|          9|     3|female|27.0|    0|    2|11.1333|       1| 27.0|\n",
      "|         10|     2|female|14.0|    1|    0|30.0708|       1| 14.0|\n",
      "|         11|     3|female| 4.0|    1|    1|   16.7|       1|  4.0|\n",
      "|         12|     1|female|58.0|    0|    0|  26.55|       1| 58.0|\n",
      "|         13|     3|  male|20.0|    0|    0|   8.05|       0| 20.0|\n",
      "|         14|     3|  male|39.0|    1|    5| 31.275|       0| 39.0|\n",
      "|         15|     3|female|14.0|    0|    0| 7.8542|       0| 14.0|\n",
      "|         16|     2|female|55.0|    0|    0|   16.0|       1| 55.0|\n",
      "|         17|     3|  male| 2.0|    4|    1| 29.125|       0|  2.0|\n",
      "|         18|     2|  male|null|    0|    0|   13.0|       1| null|\n",
      "|         19|     3|female|31.0|    1|    0|   18.0|       0| 31.0|\n",
      "|         20|     3|female|null|    0|    0|  7.225|       1| null|\n",
      "+-----------+------+------+----+-----+-----+-------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.na.fill('NA','AgeNA').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iii)\tPrint the revised dataframe and recalculate the summary statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------+------------------+------------------+-------------------+-----------------+-------------------+------------------+\n",
      "|summary|      PassengerId|            Pclass|   Sex|               Age|             SibSp|              Parch|             Fare|           Survived|             AgeNA|\n",
      "+-------+-----------------+------------------+------+------------------+------------------+-------------------+-----------------+-------------------+------------------+\n",
      "|  count|              891|               891|   891|               714|               891|                891|              891|                891|               714|\n",
      "|   mean|            446.0| 2.308641975308642|  null| 29.69911764705882|0.5230078563411896|0.38159371492704824| 32.2042079685746| 0.3838383838383838| 29.69911764705882|\n",
      "| stddev|257.3538420152301|0.8360712409770491|  null|14.526497332334035|1.1027434322934315| 0.8060572211299488|49.69342859718089|0.48659245426485753|14.526497332334035|\n",
      "|    min|                1|                 1|female|              0.42|                 0|                  0|              0.0|                  0|              0.42|\n",
      "|    max|              891|                 3|  male|              80.0|                 8|                  6|         512.3292|                  1|              80.0|\n",
      "+-------+-----------------+------------------+------+------------------+------------------+-------------------+-----------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : The goal of this step is to encode all string and categorical variables in order to use them in the pipeline afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i)\tImport all necessary pyspark functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the string into numerical code\n",
    "# Label encoding \n",
    "gender_encoder = StringIndexer(inputCol='Sex', outputCol='Gender').fit(filled_df) #converting sex(string) into numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ii)\tCreate indexers and encoders for categorical string variables. Call them [field]_indexer and [field]_encoder, respectively. For instance, gender_indexer and gender_encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_indexer = gender_encoder.transform(filled_df) #tansform string variables (sex) into nemerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+-----------------+-----+-----+-------+--------+------+\n",
      "|PassengerId|Pclass|   Sex|              Age|SibSp|Parch|   Fare|Survived|Gender|\n",
      "+-----------+------+------+-----------------+-----+-----+-------+--------+------+\n",
      "|          1|     3|  male|             22.0|    1|    0|   7.25|       0|   0.0|\n",
      "|          2|     1|female|             38.0|    1|    0|71.2833|       1|   1.0|\n",
      "|          3|     3|female|             26.0|    0|    0|  7.925|       1|   1.0|\n",
      "|          4|     1|female|             35.0|    1|    0|   53.1|       1|   1.0|\n",
      "|          5|     3|  male|             35.0|    0|    0|   8.05|       0|   0.0|\n",
      "|          6|     3|  male|29.69911764705882|    0|    0| 8.4583|       0|   0.0|\n",
      "|          7|     1|  male|             54.0|    0|    0|51.8625|       0|   0.0|\n",
      "|          8|     3|  male|              2.0|    3|    1| 21.075|       0|   0.0|\n",
      "|          9|     3|female|             27.0|    0|    2|11.1333|       1|   1.0|\n",
      "|         10|     2|female|             14.0|    1|    0|30.0708|       1|   1.0|\n",
      "|         11|     3|female|              4.0|    1|    1|   16.7|       1|   1.0|\n",
      "|         12|     1|female|             58.0|    0|    0|  26.55|       1|   1.0|\n",
      "|         13|     3|  male|             20.0|    0|    0|   8.05|       0|   0.0|\n",
      "|         14|     3|  male|             39.0|    1|    5| 31.275|       0|   0.0|\n",
      "|         15|     3|female|             14.0|    0|    0| 7.8542|       0|   1.0|\n",
      "|         16|     2|female|             55.0|    0|    0|   16.0|       1|   1.0|\n",
      "|         17|     3|  male|              2.0|    4|    1| 29.125|       0|   0.0|\n",
      "|         18|     2|  male|29.69911764705882|    0|    0|   13.0|       1|   0.0|\n",
      "|         19|     3|female|             31.0|    1|    0|   18.0|       0|   1.0|\n",
      "|         20|     3|female|29.69911764705882|    0|    0|  7.225|       1|   1.0|\n",
      "+-----------+------+------+-----------------+-----+-----+-------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gender_indexer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['male', 'female']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the labels \n",
    "gender_encoder.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PassengerId', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Survived', 'Gender']\n"
     ]
    }
   ],
   "source": [
    "print(gender_indexer.columns) #check the columns to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_features = ['PassengerId', 'Pclass', 'Gender', 'Age', 'SibSp', 'Parch', 'Fare']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: The goal of this step is to assemble all feature columns into a feature vector in order to be used in the pipeline. Tip: you can use the VectorAssembler to do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorAssembler\n",
    "vec_assembler = VectorAssembler( inputCols=required_features, outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_assembler = VectorAssembler( inputCols=required_features, outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_df = vec_assembler.transform(gender_indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[PassengerId: int, Pclass: int, Sex: string, Age: double, SibSp: int, Parch: int, Fare: double, Survived: int, Gender: double, features: vector]\n"
     ]
    }
   ],
   "source": [
    "print (vec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = vec_df.select('features', 'Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|            features|Survived|\n",
      "+--------------------+--------+\n",
      "|[1.0,3.0,0.0,22.0...|       0|\n",
      "|[2.0,1.0,1.0,38.0...|       1|\n",
      "|[3.0,3.0,1.0,26.0...|       1|\n",
      "|[4.0,1.0,1.0,35.0...|       1|\n",
      "|[5.0,3.0,0.0,35.0...|       0|\n",
      "|[6.0,3.0,0.0,29.6...|       0|\n",
      "|[7.0,1.0,0.0,54.0...|       0|\n",
      "|[8.0,3.0,0.0,2.0,...|       0|\n",
      "|[9.0,3.0,1.0,27.0...|       1|\n",
      "|[10.0,2.0,1.0,14....|       1|\n",
      "|[11.0,3.0,1.0,4.0...|       1|\n",
      "|[12.0,1.0,1.0,58....|       1|\n",
      "|[13.0,3.0,0.0,20....|       0|\n",
      "|[14.0,3.0,0.0,39....|       0|\n",
      "|[15.0,3.0,1.0,14....|       0|\n",
      "|[16.0,2.0,1.0,55....|       1|\n",
      "|[17.0,3.0,0.0,2.0...|       0|\n",
      "|[18.0,2.0,0.0,29....|       1|\n",
      "|[19.0,3.0,1.0,31....|       0|\n",
      "|[20.0,3.0,1.0,29....|       1|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 : The goal of this step is to create the logistic regression model to be used in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol='features',labelCol='Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Step 7 : The goal of this step is to assemble the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8 : The goal of this step is to prepare the training and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i)Use a 70-30 random split for the training and test sets, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_final.randomSplit([0.7,0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the size of each dataset after the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "659"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Step 9 : The goal of this step is to fit the model and then use it on the test set to generate\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i)\tFit the model using the predefined pipeline on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii)\tUse the fitted model for prediction on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii)\tReport the logistic regression coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+----------+\n",
      "|            features|Survived|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------+--------------------+--------------------+----------+\n",
      "|(7,[0,1,3],[272.0...|       1|[2.15425375627881...|[0.89606560582915...|       0.0|\n",
      "|(7,[0,1,3],[467.0...|       0|[1.21169147821365...|[0.77059809978306...|       0.0|\n",
      "|(7,[0,1,3],[598.0...|       0|[3.11283037468230...|[0.95741889387706...|       0.0|\n",
      "|(7,[0,1,3],[634.0...|       0|[0.07827835357711...|[0.51955960179140...|       0.0|\n",
      "|(7,[0,1,3],[816.0...|       0|[0.06220230289395...|[0.51554556373398...|       0.0|\n",
      "|(7,[0,1,3],[823.0...|       0|[0.40308650654328...|[0.59942900007551...|       0.0|\n",
      "|[1.0,3.0,0.0,22.0...|       0|[2.39000683199169...|[0.91606209343172...|       0.0|\n",
      "|[5.0,3.0,0.0,35.0...|       0|[2.57245456948041...|[0.92906762489188...|       0.0|\n",
      "|[7.0,1.0,0.0,54.0...|       0|[1.02525208198976...|[0.73599437804879...|       0.0|\n",
      "|[9.0,3.0,1.0,27.0...|       1|[-0.5208176768165...|[0.37266105338633...|       1.0|\n",
      "|[10.0,2.0,1.0,14....|       1|[-2.0452186638291...|[0.11453640095594...|       1.0|\n",
      "|[14.0,3.0,0.0,39....|       0|[3.49235982055951...|[0.97046959953320...|       0.0|\n",
      "|[15.0,3.0,1.0,14....|       0|[-1.2310224725029...|[0.22600251923293...|       1.0|\n",
      "|[16.0,2.0,1.0,55....|       1|[-0.6800001659500...|[0.33626126555731...|       1.0|\n",
      "|[18.0,2.0,0.0,29....|       1|[1.22423998982698...|[0.77280884802555...|       0.0|\n",
      "|[20.0,3.0,1.0,29....|       1|[-0.5842822506694...|[0.35794784422054...|       1.0|\n",
      "|[21.0,2.0,0.0,35....|       0|[1.41494435515546...|[0.80454462712703...|       0.0|\n",
      "|[25.0,3.0,1.0,8.0...|       0|[-0.3644061276993...|[0.40989338668536...|       1.0|\n",
      "|[28.0,1.0,0.0,19....|       0|[0.37590814591359...|[0.59288581963037...|       0.0|\n",
      "|[29.0,3.0,1.0,29....|       1|[-0.5864415611225...|[0.35745174129809...|       1.0|\n",
      "+--------------------+--------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.ml.classification.BinaryLogisticRegressionTrainingSummary at 0x145472b00a0>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrm_summary = lr_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+----------+\n",
      "|            features|Survived|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------+--------------------+--------------------+----------+\n",
      "|(7,[0,1,3],[180.0...|     0.0|[2.61492569485528...|[0.93181601637760...|       0.0|\n",
      "|(7,[0,1,3],[264.0...|     0.0|[0.53474396307020...|[0.63058888472780...|       0.0|\n",
      "|(7,[0,1,3],[278.0...|     0.0|[1.22838583853847...|[0.77353593339872...|       0.0|\n",
      "|(7,[0,1,3],[303.0...|     0.0|[1.90467248243697...|[0.87041944504089...|       0.0|\n",
      "|(7,[0,1,3],[414.0...|     0.0|[1.21637296550050...|[0.77142462895078...|       0.0|\n",
      "|(7,[0,1,3],[482.0...|     0.0|[1.21036652898152...|[0.77036379570154...|       0.0|\n",
      "|(7,[0,1,3],[675.0...|     0.0|[1.19331884886146...|[0.76733411215394...|       0.0|\n",
      "|(7,[0,1,3],[733.0...|     0.0|[1.18819571183057...|[0.76641821343115...|       0.0|\n",
      "|(7,[0,1,3],[807.0...|     0.0|[0.44564029329568...|[0.60960217603063...|       0.0|\n",
      "|[2.0,1.0,1.0,38.0...|     1.0|[-2.2617509637085...|[0.09434065943680...|       1.0|\n",
      "|[3.0,3.0,1.0,26.0...|     1.0|[-0.7364240763971...|[0.32378659419467...|       1.0|\n",
      "|[4.0,1.0,1.0,35.0...|     1.0|[-2.3474276847821...|[0.08727045104418...|       1.0|\n",
      "|[6.0,3.0,0.0,29.6...|     0.0|[2.35343373505234...|[0.91320677239095...|       0.0|\n",
      "|[8.0,3.0,0.0,2.0,...|     0.0|[2.32930386243559...|[0.91127506817345...|       0.0|\n",
      "|[11.0,3.0,1.0,4.0...|     1.0|[-1.2193216962592...|[0.22805584146016...|       1.0|\n",
      "|[12.0,1.0,1.0,58....|     1.0|[-1.6968894815096...|[0.15487195345199...|       1.0|\n",
      "|[13.0,3.0,0.0,20....|     0.0|[1.95464031631801...|[0.87595174018044...|       0.0|\n",
      "|[17.0,3.0,0.0,2.0...|     0.0|[2.66207766686404...|[0.93475150034971...|       0.0|\n",
      "|[19.0,3.0,1.0,31....|     0.0|[-0.2027892023444...|[0.44947572527668...|       1.0|\n",
      "|[22.0,2.0,0.0,34....|     1.0|[1.40082715303689...|[0.80231511244625...|       0.0|\n",
      "+--------------------+--------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrm_summary.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+\n",
      "|summary|           Survived|        prediction|\n",
      "+-------+-------------------+------------------+\n",
      "|  count|                659|               659|\n",
      "|   mean|0.38239757207890746|0.3566009104704097|\n",
      "| stddev|0.48634204981328594|0.4793593528009135|\n",
      "|    min|                0.0|               0.0|\n",
      "|    max|                1.0|               1.0|\n",
      "+-------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrm_summary.predictions.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Step 10 : The goal of this step is to evaluate the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = lr_model.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 5 rows of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+----------+\n",
      "|            features|Survived|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------+--------------------+--------------------+----------+\n",
      "|(7,[0,1,3],[272.0...|       1|[2.15425375627881...|[0.89606560582915...|       0.0|\n",
      "|(7,[0,1,3],[467.0...|       0|[1.21169147821365...|[0.77059809978306...|       0.0|\n",
      "|(7,[0,1,3],[598.0...|       0|[3.11283037468230...|[0.95741889387706...|       0.0|\n",
      "|(7,[0,1,3],[634.0...|       0|[0.07827835357711...|[0.51955960179140...|       0.0|\n",
      "|(7,[0,1,3],[816.0...|       0|[0.06220230289395...|[0.51554556373398...|       0.0|\n",
      "+--------------------+--------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_labels.predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii)\tReport the AUC for this model. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC =eval.evaluate(pred_labels.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7447574334898278"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our AUC is 75% which is acceptable, it would implicate the great performance of the model as it is above 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d9902862e57d62f901ca8e6311aaf4900390219de8e40113029502fcbe1dae2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
